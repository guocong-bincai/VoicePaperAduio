#!/usr/bin/env python3
"""
VoxCPM æ–‡æœ¬åˆ†å‰²å·¥å…· - å¤„ç†è¶…é•¿æ–‡æœ¬
å°†å¤§äº 4096 Token çš„æ–‡æœ¬è‡ªåŠ¨åˆ†å‰²æˆå¯å¤„ç†çš„å—
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

def split_by_sentences(text: str, max_chars: int = 2000, language: str = "auto"):
    """
    æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬

    Args:
        text: è¾“å…¥æ–‡æœ¬
        max_chars: æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
        language: 'zh' (ä¸­æ–‡), 'en' (è‹±æ–‡), 'auto' (è‡ªåŠ¨æ£€æµ‹)

    Returns:
        list: åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
    """
    if language == "auto":
        # ç®€å•çš„è‡ªåŠ¨æ£€æµ‹
        if any('\u4e00' <= c <= '\u9fff' for c in text):
            language = "zh"
        else:
            language = "en"

    chunks = []
    current_chunk = ""

    if language == "zh":
        # ä¸­æ–‡ï¼šæŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
        sentences = []
        temp = ""
        for char in text:
            temp += char
            if char in 'ã€‚ï¼ï¼Ÿ\n':
                sentences.append(temp)
                temp = ""
        if temp:
            sentences.append(temp)

        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= max_chars:
                current_chunk += sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence

    else:  # è‹±æ–‡æˆ–å…¶ä»–
        # è‹±æ–‡ï¼šæŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
        sentences = []
        temp = ""
        for char in text:
            temp += char
            if char in '.!?\n':
                sentences.append(temp)
                temp = ""
        if temp:
            sentences.append(temp)

        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= max_chars:
                current_chunk += sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence

    if current_chunk:
        chunks.append(current_chunk)

    return chunks


def estimate_tokens(text: str, language: str = "auto"):
    """
    ä¼°ç®—æ–‡æœ¬éœ€è¦çš„ Token æ•°

    Args:
        text: è¾“å…¥æ–‡æœ¬
        language: è¯­è¨€

    Returns:
        int: ä¼°ç®—çš„ Token æ•°
    """
    if language == "auto":
        if any('\u4e00' <= c <= '\u9fff' for c in text):
            language = "zh"
        else:
            language = "en"

    if language == "zh":
        # ä¸­æ–‡ï¼šå¤§çº¦ 1 å­—ç¬¦ â‰ˆ 1 Token
        return len(text)
    else:
        # è‹±æ–‡ï¼šå¤§çº¦ 1 å­—ç¬¦ â‰ˆ 0.25 Token (ç²—ç•¥ä¼°ç®—)
        return int(len(text) * 0.25)


def format_output(chunks: list):
    """æ ¼å¼åŒ–è¾“å‡ºåˆ†å‰²ç»“æœ"""
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æ–‡æœ¬åˆ†å‰²ç»“æœ")
    print("=" * 60)
    print(f"æ€»å—æ•°ï¼š{len(chunks)}")
    print(f"å­—ç¬¦æ•°ï¼š{sum(len(c) for c in chunks)}")

    for i, chunk in enumerate(chunks, 1):
        tokens = estimate_tokens(chunk)
        print(f"\nã€å— {i}ã€‘")
        print(f"  å­—ç¬¦æ•°ï¼š{len(chunk)}")
        print(f"  ä¼°ç®— Tokenï¼š{tokens}")
        print(f"  é¢„è§ˆï¼š{chunk[:100]}{'...' if len(chunk) > 100 else ''}")

    print("\n" + "=" * 60)
    print("âœ… åˆ†å‰²å®Œæˆ")
    print("=" * 60)


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="VoxCPM æ–‡æœ¬åˆ†å‰²å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  # ä»å‘½ä»¤è¡Œè¾“å…¥
  python text_splitter.py --text "å¾ˆé•¿çš„ä¸­æ–‡æ–‡æœ¬..."

  # ä»æ–‡ä»¶è¯»å–
  python text_splitter.py --file input.txt

  # æŒ‡å®šæœ€å¤§é•¿åº¦
  python text_splitter.py --file input.txt --max-chars 1500

  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
  python text_splitter.py --text "æ··åˆ text ä¸­æ–‡"
        """
    )

    parser.add_argument("--text", type=str, help="è¾“å…¥æ–‡æœ¬ï¼ˆç›´æ¥ï¼‰")
    parser.add_argument("--file", type=str, help="è¾“å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max-chars", type=int, default=2000, help="æ¯å—æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤ 2000ï¼‰")
    parser.add_argument("--language", choices=["zh", "en", "auto"], default="auto", help="è¯­è¨€ï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹ï¼‰")
    parser.add_argument("--save", type=str, help="ä¿å­˜åˆ†å‰²ç»“æœåˆ°æ–‡ä»¶ï¼ˆJSON æ ¼å¼ï¼‰")

    args = parser.parse_args()

    # è·å–è¾“å…¥æ–‡æœ¬
    if args.text:
        text = args.text
    elif args.file:
        try:
            with open(args.file, 'r', encoding='utf-8') as f:
                text = f.read()
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°ï¼š{args.file}")
            return 1
    else:
        print("âŒ è¯·ä½¿ç”¨ --text æˆ– --file æŒ‡å®šè¾“å…¥")
        parser.print_help()
        return 1

    # åˆ†å‰²æ–‡æœ¬
    chunks = split_by_sentences(text, max_chars=args.max_chars, language=args.language)

    # è¾“å‡ºç»“æœ
    format_output(chunks)

    # ä¿å­˜ç»“æœ
    if args.save:
        import json
        result = {
            "total_chunks": len(chunks),
            "total_chars": sum(len(c) for c in chunks),
            "estimated_tokens": sum(estimate_tokens(c, args.language) for c in chunks),
            "chunks": [
                {
                    "index": i,
                    "text": chunk,
                    "chars": len(chunk),
                    "tokens": estimate_tokens(chunk, args.language)
                }
                for i, chunk in enumerate(chunks, 1)
            ]
        }
        with open(args.save, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°ï¼š{args.save}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
